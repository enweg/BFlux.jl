<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MCMC Estimation · BayesFlux.jl Documentation</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BayesFlux.jl Documentation</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../">BayesFlux.jl a Bayesian extension to Flux.jl</a></li><li><a class="tocitem" href="../../introduction/linear-regression/">Example: Linear Regression</a></li><li><a class="tocitem" href="../../introduction/feedforward/">Example: Feedforward NN Regression</a></li><li><a class="tocitem" href="../../introduction/recurrent/">Example: Recurrent Neural Networks</a></li></ul></li><li><span class="tocitem">Bayesian Neural Networks</span><ul><li><a class="tocitem" href="../../model/bnn/">Model Basics</a></li><li><a class="tocitem" href="../../model/sampling/">Prior and Posterior Predictive</a></li></ul></li><li><span class="tocitem">Likelihood Functions</span><ul><li><a class="tocitem" href="../../likelihoods/interface/">Interface</a></li><li><a class="tocitem" href="../../likelihoods/feedforward/">Feedforward Likelihoods</a></li><li><a class="tocitem" href="../../likelihoods/seqtoone/">Seq-to-One Likelihoods</a></li></ul></li><li><span class="tocitem">Network Priors</span><ul><li><a class="tocitem" href="../../priors/interface/">Interface</a></li><li><a class="tocitem" href="../../priors/gaussian/">Gaussian Prior</a></li><li><a class="tocitem" href="../../priors/mixturescale/">Mixture Scale Prior</a></li></ul></li><li><span class="tocitem">Inference</span><ul><li><a class="tocitem" href="../map/">MAP Estimation</a></li><li class="is-active"><a class="tocitem" href>MCMC Estimation</a><ul class="internal"><li><a class="tocitem" href="#Sampler"><span>Sampler</span></a></li><li><a class="tocitem" href="#Mass-Adaptation"><span>Mass Adaptation</span></a></li><li><a class="tocitem" href="#Stepsize-Adaptation"><span>Stepsize Adaptation</span></a></li></ul></li><li><a class="tocitem" href="../vi/">Variational Inference</a></li></ul></li><li><span class="tocitem">Initialisation</span><ul><li><a class="tocitem" href="../../initialise/init/">Initialisation</a></li></ul></li><li><span class="tocitem">Utils</span><ul><li><a class="tocitem" href="../../utils/recurrent/">Recurrent Architectures</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Inference</a></li><li class="is-active"><a href>MCMC Estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MCMC Estimation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/enweg/BayesFlux.jl/blob/main/docs/src/inference/mcmc.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="MCMC-Estimation"><a class="docs-heading-anchor" href="#MCMC-Estimation">MCMC Estimation</a><a id="MCMC-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-Estimation" title="Permalink"></a></h1><h2 id="Sampler"><a class="docs-heading-anchor" href="#Sampler">Sampler</a><a id="Sampler-1"></a><a class="docs-heading-anchor-permalink" href="#Sampler" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.MCMCState" href="#BayesFlux.MCMCState"><code>BayesFlux.MCMCState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">abstract type MCMCState end</code></pre><p>Every MCMC method must be implemented via a MCMCState which keeps track of all  important information. </p><p><strong>Mandatory Fields</strong></p><ul><li><code>samples::Matrix</code> of dimension num<em>total</em>parameter×num_samples</li><li><code>nsampled</code> the number of thus far sampled samples</li></ul><p><strong>Mandatory Functions</strong></p><ul><li><code>update!(sampler, θ, bnn, ∇θ)</code> where θ is the current parameter vector and ∇θ(θ) is a function providing gradients. The function must return θ and num_samples so far sampled. </li><li><code>initialise!(sampler, θ, numsamples; continue_sampling)</code> which initialises the sampler. If continue_sampling is true, then the final goal is to obtain numsamples samples and thus only the remaining ones still need to be sampled.</li><li><code>calculate_epochs(sampler, numbatches, numsamples; continue_sampling)</code> which calculates the number of epochs that must be run through in order to obtain <code>numsamples</code> samples if <code>numbatches</code> batches are used. The number of epochs must be returned. If <code>continue_sampling</code> is true, then the goal is to obtain in total <code>numsamples</code> samples and thus we only need the number of epochs that still need to be run to obtain this total and NOT the number of epochs to sample <code>numsamples</code> new samples.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/abstract.jl#L2-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.mcmc" href="#BayesFlux.mcmc"><code>BayesFlux.mcmc</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mcmc(args...; kwargs...)</code></pre><p>Sample from a BNN using MCMC</p><p><strong>Arguments</strong></p><ul><li><code>bnn</code>: a Bayesian Neural Network</li><li><code>batchsize</code>: batchsize</li><li><code>numsamples</code>: number of samples to take</li><li><code>sampler</code>: sampler to use </li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>shuffle::Bool=true</code>: should data be shuffled after each epoch such that batches are different in each epoch? </li><li><code>partial::Bool=true</code>: are partial batches allowed? If true, some batches might be smaller than <code>batchsize</code></li><li><code>showprogress::Bool=true</code>: should a progress bar be shown? </li><li><code>continue_sampling::Bool=false</code>: If true and <code>numsamples</code> is larger than <code>sampler.nsampled</code> then additional samples will be taken </li><li><code>θstart::AbstractVector{T}=vcat(bnn.init()...)</code>: starting parameter vector</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/abstract.jl#L50-L73">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.SGLD" href="#BayesFlux.SGLD"><code>BayesFlux.SGLD</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Stochastic Gradient Langevin Dynamics as proposed in Welling, M., &amp; Teh, Y. W. (n.d.). Bayesian Learning via Stochastic Gradient Langevin Dynamics. 8.</p><p><strong>Fields</strong></p><ul><li><code>θ::AbstractVector</code>: Current sample</li><li><code>samples::Matrix</code>: Matrix of samples. Not all columns will be actual samples if sampling was stopped early. See <code>nsampled</code> for the actual number of samples taken. </li><li><code>nsampled::Int</code>: Number of samples taken</li><li><code>min_stepsize::T</code>: Stop decreasing the stepsize when it is below this value. </li><li><code>didinform::Bool</code>: Flag keeping track of whether we informed user that <code>min_stepsize</code> was reached. </li><li><code>stepsize_a::T</code>: See <code>stepsize</code></li><li><code>stepsize_b::T</code>: See <code>stepsize</code></li><li><code>stepsize_γ::T</code>: See <code>stepsize</code></li><li><code>maxnorm::T</code>: Maximimum gradient norm. Gradients are being clipped if norm exceeds this value</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/sgld.jl#L14-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.SGNHTS" href="#BayesFlux.SGNHTS"><code>BayesFlux.SGNHTS</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Stochastic Gradient Nose-Hoover Thermostat as proposed in </p><p>Proposed in Leimkuhler, B., &amp; Shang, X. (2016). Adaptive thermostats for noisy gradient systems. SIAM Journal on Scientific Computing, 38(2), A712-A736.</p><p>This is similar to SGNHT as proposed in  Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., &amp; Neven, H. (2014). Bayesian sampling using stochastic gradient thermostats. Advances in neural information processing systems, 27.</p><p><strong>Fields</strong></p><ul><li><code>samples::Matrix</code>: Containing the samples </li><li><code>nsampled::Int</code>: Number of samples taken so far. Can be smaller than <code>size(samples, 2)</code> if sampling was interrupted. </li><li><code>p::AbstractVector</code>: Momentum</li><li><code>xi::Number</code>: Thermostat</li><li><code>l::Number</code>: Stepsize; This often is in the 0.001-0.1 range.</li><li><code>σA::Number</code>: Diffusion factor; If the stepsize is small, this should be larger than 1.</li><li><code>μ::Number</code>: Free parameter in thermostat. Defaults to 1.</li><li><code>t::Int</code>: Current step count</li><li><code>kinetic::Vector</code>: Keeps track of the kinetic energy. Goal of SGNHT is to have the average close to one</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/sgnht-s.jl#L2-L25">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.SGNHT" href="#BayesFlux.SGNHT"><code>BayesFlux.SGNHT</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Stochastic Gradient Nose-Hoover Thermostat as proposed in </p><p>Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., &amp; Neven, H. (2014). Bayesian sampling using stochastic gradient thermostats. Advances in neural information processing systems, 27.</p><p><strong>Fields</strong></p><ul><li><code>samples::Matrix</code>: Containing the samples </li><li><code>nsampled::Int</code>: Number of samples taken so far. Can be smaller than <code>size(samples, 2)</code> if sampling was interrupted. </li><li><code>p::AbstractVector</code>: Momentum</li><li><code>xi::Number</code>: Thermostat</li><li><code>l::Number</code>: Stepsize</li><li><code>A::Number</code>: Diffusion factor</li><li><code>t::Int</code>: Current step count</li><li><code>kinetic::Vector</code>: Keeps track of the kinetic energy. Goal of SGNHT is to have the average close to one</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/sgnht.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.GGMC" href="#BayesFlux.GGMC"><code>BayesFlux.GGMC</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Gradient Guided Monte Carlo</p><p>Proposed in Garriga-Alonso, A., &amp; Fortuin, V. (2021). Exact langevin dynamics with stochastic gradients. arXiv preprint arXiv:2102.01691.</p><p><strong>Fields</strong></p><ul><li><code>samples::Matrix</code>: Matrix containing the samples. If sampling stopped early, then not all columns will actually correspond to samples. See <code>nsampled</code> to check how many samples were actually taken</li><li><code>nsampled::Int</code>: Number of samples taken. </li><li><code>t::Int</code>: Total number of steps taken.</li><li><code>accepted::Vector{Bool}</code>: If true, sample was accepted; If false, proposed sample was rejected and previous sample was taken. </li><li><code>β::T</code>: See paper. </li><li><code>l::T</code>: Step-length; See paper.</li><li><code>sadapter::StepsizeAdapter</code>: A StepsizeAdapter. Default is <code>DualAveragingStepSize</code></li><li><code>M::AbstractMatrix</code>: Mass Matrix</li><li><code>Mhalf::AbstractMatrix</code>: Lower triangual cholesky decomposition of <code>M</code></li><li><code>Minv::AbstractMatrix</code>: Inverse mass matrix.</li><li><code>madapter::MassAdapter</code>: A MassAdapter</li><li><code>momentum::AbstractVector</code>: Last momentum vector</li><li><code>lMH::T</code>: log of Metropolis-Hastings ratio. </li><li><code>steps::Int</code>: Number of steps to take before calculating MH ratio. </li><li><code>current_step::Int</code>: Current step in the recurrent sequence 1, ..., <code>steps</code>. </li><li><code>maxnorm::T</code>: Maximimum gradient norm. Gradients are being clipped if norm exceeds this value</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/ggmc.jl#L3-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.HMC" href="#BayesFlux.HMC"><code>BayesFlux.HMC</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Standard Hamiltonian Monte Carlo (Hybrid Monte Carlo).</p><p>Allows for the use of stochastic gradients, but the validity of doing so is not clear. </p><p>This is motivated by parts of the discussion in  Neal, R. M. (1996). Bayesian Learning for Neural Networks (Vol. 118). Springer New York. https://doi.org/10.1007/978-1-4612-0745-0</p><p>Code was partially adapted from https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/</p><p><strong>Fields</strong></p><ul><li><code>samples::Matrix</code>: Samples taken</li><li><code>nsampled::Int</code>: Number of samples taken. Might be smaller than <code>size(samples)</code> if sampling was interrupted.</li><li><code>θold::AbstractVector</code>: Old sample. Kept for rejection step.</li><li><code>momentum::AbstractVector</code>: Momentum variables</li><li><code>momentumold::AbstractVector</code>: Old momentum variables. Kept for rejection step.</li><li><code>t::Int</code>: Current step.</li><li><code>path_len::Int</code>: Number of leapfrog steps. </li><li><code>current_step::Int</code>: Current leapfrog step.</li><li><code>accepted::Vector{Bool}</code>: Whether a draw in <code>samples</code> was a accepted draw or rejected (in which case it is the same as the previous one.)</li><li><code>sadapter::StepsizeAdapter</code>: Stepsize adapter giving the stepsize in each iteration.</li><li><code>l</code>: Stepsize.</li><li><code>madapter::MassAdapter</code>: Mass matrix adapter giving the inverse mass matrix in each iteration.</li><li><code>Minv::AbstractMatrix</code>: Inverse mass matrix</li><li><code>maxnorm::T</code>: Maximimum gradient norm. Gradients are being clipped if norm exceeds this value</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/hmc.jl#L3-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.AdaptiveMH" href="#BayesFlux.AdaptiveMH"><code>BayesFlux.AdaptiveMH</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Adaptive Metropolis Hastings as introduced in </p><p>Haario, H., Saksman, E., &amp; Tamminen, J. (2001). An adaptive Metropolis algorithm. Bernoulli, 223-242.</p><p><strong>Fields</strong></p><ul><li><code>samples::Matix</code>: Matrix holding the samples. If sampling was stopped early, not all columns will represent samples. To figure out how many columns represent samples, check out <code>nsampled</code>.</li><li><code>nsampled::Int</code>: Number of samples obtained.</li><li><code>C0::Matrix</code>: Initial covariance matrix. </li><li><code>Ct::Matrix</code>: Covariance matrix in iteration t </li><li><code>t::Int</code>: Current time period</li><li><code>t0::Int</code>: When to start adaptig the covariance matrix? Covariance is adapted in a rolling window form. </li><li><code>sd::T</code>: See the paper. </li><li><code>ϵ::T</code>: Will be added to diagonal to prevent numerical non-pod-def problems. If you run into numerical problems, try increasing this values.</li><li><code>accepted::Vector{Bool}</code>: For each sample, indicating whether the sample was accepted (true) or the previous samples was chosen (false)</li></ul><p><strong>Notes</strong></p><ul><li>Adaptive MH might not be suited if it is very costly to calculate the likelihood as this needs to be done for each sample on the full dataset. Plans exist to make this faster. </li><li>Works best when started at a MAP estimate. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/amh.jl#L1-L30">source</a></section></article><h2 id="Mass-Adaptation"><a class="docs-heading-anchor" href="#Mass-Adaptation">Mass Adaptation</a><a id="Mass-Adaptation-1"></a><a class="docs-heading-anchor-permalink" href="#Mass-Adaptation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.MassAdapter" href="#BayesFlux.MassAdapter"><code>BayesFlux.MassAdapter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Adapt the mass matrix in MCMC and especially dynamic MCMC methods such as   HMC, GGMC, SGLD, SGNHT, ...</p><p><strong>Mandatory Fields</strong></p><ul><li><code>Minv::AbstractMatrix</code>: The inverse mass matrix used in HMC, GGMC, ...</li></ul><p><strong>Mandatory Functions</strong></p><ul><li><code>(madapter::MassAdapter)(s::MCMCState, θ::AbstractVector, bnn, ∇θ)</code>: Every mass adapter must be callable and have the sampler state, the current sample, the BNN and a gradient function as arguments. It must return the new <code>Minv</code> Matrix.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/mass/abstract_mass.jl#L3-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.DiagCovMassAdapter" href="#BayesFlux.DiagCovMassAdapter"><code>BayesFlux.DiagCovMassAdapter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use the variances as the diagonal of the inverse mass matrix as used in HMC,  GGMC, ...; </p><p><strong>Fields</strong></p><ul><li><code>Minv</code>: Inverse mass matrix as used in HMC, SGLD, GGMC, ...</li><li><code>adapt_steps</code>: Number of adaptation steps. </li><li><code>windowlength</code>: Lookback length for calculation of covariance.</li><li><code>t</code>: Current step. </li><li><code>kappa</code>: How much to shrink towards the identity.</li><li><code>epsilon</code>: Small value to add to diagonal to avoid numerical instability.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/mass/diagcovariancemassadapter.jl#L1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.FullCovMassAdapter" href="#BayesFlux.FullCovMassAdapter"><code>BayesFlux.FullCovMassAdapter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use the full covariance matrix of a moving average of samples as the mass matrix. This is similar to what is already done in Adaptive MH.</p><p><strong>Fields</strong></p><ul><li><code>Minv</code>: Inverse mass matrix as used in HMC, SGLD, GGMC, ...</li><li><code>adapt_steps</code>: Number of adaptation steps. </li><li><code>windowlength</code>: Lookback length for calculation of covariance.</li><li><code>t</code>: Current step.</li><li><code>kappa</code>: How much to shrink towards the identity.</li><li><code>epsilon</code>: Small value to add to diagonal to avoid numerical instability.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/mass/fullcovariancemassadapter.jl#L2-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.FixedMassAdapter" href="#BayesFlux.FixedMassAdapter"><code>BayesFlux.FixedMassAdapter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use a fixed inverse mass matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/mass/fixedmassmatrix.jl#L2-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.RMSPropMassAdapter" href="#BayesFlux.RMSPropMassAdapter"><code>BayesFlux.RMSPropMassAdapter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use RMSProp as a precondition/mass matrix adapter. This was proposed in </p><p>Li, C., Chen, C., Carlson, D., &amp; Carin, L. (2016, February). Preconditioned stochastic gradient Langevin dynamics for deep neural networks. In Thirtieth AAAI Conference on Artificial Intelligence for the use in SGLD and related methods. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/mass/rmspropmassadapter.jl#L1-L8">source</a></section></article><h2 id="Stepsize-Adaptation"><a class="docs-heading-anchor" href="#Stepsize-Adaptation">Stepsize Adaptation</a><a id="Stepsize-Adaptation-1"></a><a class="docs-heading-anchor-permalink" href="#Stepsize-Adaptation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.StepsizeAdapter" href="#BayesFlux.StepsizeAdapter"><code>BayesFlux.StepsizeAdapter</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Adapt the stepsize of MCMC algorithms.</p><p><strong>Implentation Details</strong></p><p><strong>Mandatory Fields</strong></p><ul><li><code>l::Number</code> The stepsize. Will be used by the sampler.</li></ul><p><strong>Mandatory Functions</strong></p><ul><li><code>(sadapter::StepsizeAdapter)(s::MCMCState, mh_probability::Number)</code> Every stepsize adapter must be callable with arguments, being the sampler itself and the Metropolis-Hastings acceptance probability. The method must return the new stepsize.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/stepsize/abstract_stepsize.jl#L2-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.ConstantStepsize" href="#BayesFlux.ConstantStepsize"><code>BayesFlux.ConstantStepsize</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use a contant stepsize.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/stepsize/constantstepsize.jl#L2-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="BayesFlux.DualAveragingStepSize" href="#BayesFlux.DualAveragingStepSize"><code>BayesFlux.DualAveragingStepSize</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Use the Dual Average method to tune the stepsize.</p><p>The use of the Dual Average method was proposed in:</p><p>Hoffman, M. D., &amp; Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15, 31.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/enweg/BayesFlux.jl/blob/0774a36744e4498e045398fc59114fe40a0cd7f3/src/inference/mcmc/adapters/stepsize/dualaveragestepsize.jl#L1-L10">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../map/">« MAP Estimation</a><a class="docs-footer-nextpage" href="../vi/">Variational Inference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 5 March 2023 14:19">Sunday 5 March 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

var documenterSearchIndex = {"docs":
[{"location":"model/sampling/#Prior-and-Posterior-Predictive","page":"Prior and Posterior Predictive","title":"Prior and Posterior Predictive","text":"","category":"section"},{"location":"model/sampling/","page":"Prior and Posterior Predictive","title":"Prior and Posterior Predictive","text":"sample_prior_predictive\nget_posterior_networks\nsample_posterior_predict","category":"page"},{"location":"model/sampling/#BayesFlux.sample_prior_predictive","page":"Prior and Posterior Predictive","title":"BayesFlux.sample_prior_predictive","text":"sample_prior_predictive(bnn::BNN, predict::Function, n::Int = 1;\n\nSamples from the prior predictive. \n\nArguments\n\nbnn a BNN\npredict a function taking a network and returning a vector of predictions\nn number of samples\n\nOptional Arguments\n\nrng a RNG\n\n\n\n\n\n","category":"function"},{"location":"model/sampling/#BayesFlux.get_posterior_networks","page":"Prior and Posterior Predictive","title":"BayesFlux.get_posterior_networks","text":"get_posterior_networks(bnn::BNN, ch::AbstractMatrix{T}) where {T}\n\nGet the networks corresponding to posterior draws.\n\nArguments\n\nbnn a BNN\nch A Matrix of draws (columns are θ)\n\n\n\n\n\n","category":"function"},{"location":"model/sampling/#BayesFlux.sample_posterior_predict","page":"Prior and Posterior Predictive","title":"BayesFlux.sample_posterior_predict","text":"sample_posterior_predict(bnn::BNN, ch::AbstractMatrix{T}; x = bnn.x)\n\nSample from the posterior predictive distribution. \n\nArguments\n\nbnn: a Bayesian Neural Network \nch: draws from the posterior. These should be either obtained using mcmc or bbb \nx: explanatory variables. Default is to use the training data.\n\n\n\n\n\n","category":"function"},{"location":"introduction/feedforward/#Example:-Feedforward-NN-Regression","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"","category":"section"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"Let's say we have the same setting as in Example: Linear Regression but are not aware that it is a linear model and thus decide to use a Feedforward Neural Network. ","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"k = 5\nn = 500\nx = randn(Float32, k, n);\nβ = randn(Float32, k);\ny = x'*β + randn(Float32, n);","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"While some might think this will change a lot, given that the model we are estimating is a lot more complicated than a linear regression model, BayesFlux abstracts away all of this and all that changes is the network definition. ","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"net = Chain(Dense(k, k, relu), Dense(k, k, relu), Dense(k, 1))","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"We can then still use the same prior, likelihood, and initialiser. But we do need to change the NetworkConstructor, which we still obtain in the same way by calling destruct","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"nc = destruct(net)\nlike = FeedforwardNormal(nc, Gamma(2.0, 0.5))\nprior = GaussianPrior(nc, 0.5f0)\ninit = InitialiseAllSame(Normal(0.0f0, 0.5f0), like, prior)\nbnn = BNN(x, y, like, prior, init)","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"The rest is the same as for the linear regression case. We can, for example, first find the MAP:","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"opt = FluxModeFinder(bnn, Flux.ADAM())  # We will use ADAM\nθmap = find_mode(bnn, 10, 500, opt)  # batchsize 10 with 500 epochs","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"nethat = nc(θmap)\nyhat = vec(nethat(x))\nsqrt(mean(abs2, y .- yhat))","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"Or we can use any of the MCMC or VI method - SGNHTS is just one option:","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"sampler = SGNHTS(1f-2, 1f0; xi = 1f0^2, μ = 10f0)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"yhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"","category":"page"},{"location":"introduction/feedforward/","page":"Example: Feedforward NN Regression","title":"Example: Feedforward NN Regression","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"priors/gaussian/#Gaussian-Prior","page":"Gaussian Prior","title":"Gaussian Prior","text":"","category":"section"},{"location":"priors/gaussian/","page":"Gaussian Prior","title":"Gaussian Prior","text":"GaussianPrior","category":"page"},{"location":"priors/gaussian/#BayesFlux.GaussianPrior","page":"Gaussian Prior","title":"BayesFlux.GaussianPrior","text":"Use a Gaussian prior for all network parameters. This means that we do not allow for any correlations in the network parameters in the prior. \n\nArguments\n\nnc: obtained using destruct\nσ0: standard deviation of prior\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/seqtoone/#Seq-to-One-Likelihoods","page":"Seq-to-One Likelihoods","title":"Seq-to-One Likelihoods","text":"","category":"section"},{"location":"likelihoods/seqtoone/","page":"Seq-to-One Likelihoods","title":"Seq-to-One Likelihoods","text":"SeqToOneNormal\nSeqToOneTDist","category":"page"},{"location":"likelihoods/seqtoone/#BayesFlux.SeqToOneNormal","page":"Seq-to-One Likelihoods","title":"BayesFlux.SeqToOneNormal","text":"SeqToOneNormal(nc::NetConstructor{T, F}, prior_σ::D) where {T, F, D<:Distribution}\n\nUse a Gaussian/Normal likelihood for a Seq-to-One architecture with a single output.\n\nAssumes is a single output. Thus, the last layer must have output size one. \n\nArguments\n\nnc: obtained using destruct\nprior_σ: a prior distribution for the standard deviation\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/seqtoone/#BayesFlux.SeqToOneTDist","page":"Seq-to-One Likelihoods","title":"BayesFlux.SeqToOneTDist","text":"SeqToOneTDist(nc::NetConstructor{T, F}, prior_σ::D, ν::T) where {T, F, D}\n\nUse a Student-T likelihood for a Seq-to-One architecture with a single output and known degress of freedom.\n\nAssumes is a single output. Thus, the last layer must have output size one. \n\nArguments\n\nnc: obtained using destruct\nprior_σ: a prior distribution for the standard deviation\nν: degrees of freedom\n\n\n\n\n\n","category":"type"},{"location":"introduction/recurrent/#Example:-Recurrent-Neural-Networks","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"","category":"section"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"Next to Dense layers, BayesFlux also implements RNN and LSTM layers. These two do require some additional care though, since the layout of the data must be adjusted. In general, the last dimension of x and y is always the dimension along which BayesFlux batches, which is also what Flux does. Thus, if we are in a seq-to-one setting then the sequences must be along the last dimension (here the third). To demonstrate this, let us simulate some AR1 data","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"note: Note\nBayesFlux currently only implements univariate regression problems (a single dependent variable) and for recurrent structures only seq-to-one type of settings. This can be extended by the user. For this see BNNLikelihood","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"Random.seed!(6150533)\ngamma = 0.8\nN = 500\nburnin = 1000\ny = zeros(N + burnin + 1)\nfor t=2:(N+burnin+1)\n    y[t] = gamma*y[t-1] + randn()\nend\ny = Float32.(y[end-N+1:end])","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"Just like in the FNN case, we need a network structure and its constructor, a prior on the network parameters, a likelihood with a prior on the additional parameters introduced by the likelihood, and an initialiser. Note how most things are the same as for the FNN case, with the differences being the actual network defined and the likelihood.","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"net = Chain(RNN(1, 1), Dense(1, 1))  # last layer is linear output layer\nnc = destruct(net)\nlike = SeqToOneNormal(nc, Gamma(2.0, 0.5))\nprior = GaussianPrior(nc, 0.5f0)\ninit = InitialiseAllSame(Normal(0.0f0, 0.5f0), like, prior)","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"We are given a single sequence (time series). To exploit batching and to not always have to feed through the whole sequence, we will split the single sequence into overlapping subsequences of length 5 and store these in a tensor. Note that we add 1 to the subsequence length, because the last observation of each subsequence will be our training observation to predict using the fist five items in the subsequence.","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"x = make_rnn_tensor(reshape(y, :, 1), 5 + 1)\ny = vec(x[end, :, :])\nx = x[1:end-1, :, :]","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"We are now ready to create the BNN and find the MAP estimate. The MAP will be used to check whether the overall network structure makes sense (does provide at least good point estimates).","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"bnn = BNN(x, y, like, prior, init)\nopt = FluxModeFinder(bnn, Flux.RMSProp())\nθmap = find_mode(bnn, 10, 1000, opt)","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"When checking the performance we need to make sure to feed the sequences through the network observation by observation:","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"nethat = nc(θmap)\nyhat = vec([nethat(xx) for xx in eachslice(x; dims =1 )][end])\nsqrt(mean(abs2, y .- yhat))","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"The rest works just like before with some minor adjustments to the helper functions.","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"sampler = SGNHTS(1f-2, 1f0; xi = 1f0^2, μ = 10f0)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')\n\nfunction naive_prediction_recurrent(bnn, draws::Array{T, 2}; x = bnn.x, y = bnn.y) where {T}\n    yhats = Array{T, 2}(undef, length(y), size(draws, 2))\n    Threads.@threads for i=1:size(draws, 2)\n        net = bnn.like.nc(draws[:, i])\n        yh = vec([net(xx) for xx in eachslice(x; dims = 1)][end])\n        yhats[:,i] = yh\n    end\n    return yhats\nend","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"yhats = naive_prediction_recurrent(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"","category":"page"},{"location":"introduction/recurrent/","page":"Example: Recurrent Neural Networks","title":"Example: Recurrent Neural Networks","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/#Example:-Linear-Regression","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Although not meant for Simple Linear Regression, BayesFlux can be used for it, and we will do so in this section. This will hopefully demonstrate the basics. Later sections will show better examples.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Let's say we have the idea that the data can be modelled via a linear model of the form y_i = x_ibeta + e_i with e_i sim N(0 1)","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"k = 5\nn = 500\nx = randn(Float32, k, n);\nβ = randn(Float32, k);\ny = x'*β + randn(Float32, n);","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"This is a standard linear model and we would likely be better off using STAN or Turing for this, but due to the availability of a Dense layer with linear activation function, we can also implement it in BayesFlux.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"The first step is to define the network. As mentioned above, the network consists of a single Dense layer with a linear activation function (the default activation in Flux and hence not explicitly shown).","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"net = Chain(Dense(k, 1))  # k inputs and one output","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Since BayesFlux works with vectors, we need to be able to transform a vector to the above network and back. We thus need a NetworkConstructor, which we obtain as a the return value of a destruct","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"nc = destruct(net)","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"We can check whether everything works by just creating a random vector of the right dimension and calling the NetworkConstructor using this vector.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"θ = randn(Float32, nc.num_params_network)\nnc(θ)","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"We indeed obtain a network of the right size and structure. Next, we will define a prior for all parameters of the network. Since weight decay is a popular regularisation method in standard ML estimation, we will be using a Gaussian prior, which is the Bayesian weight decay:","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"prior = GaussianPrior(nc, 0.5f0)  # the last value is the standard deviation","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"We also need a likelihood and a prior on all parameters the likelihood introduces to the model. We will go for a Gaussian likelihood, which introduces the standard deviation of the model. BayesFlux currently implements Gaussian and Student-t likelihoods for Feedforward and Seq-to-one cases but more can easily be implemented.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"like = FeedforwardNormal(nc, Gamma(2.0, 0.5))  # Second argument is prior for standard deviation.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Lastly, when no explicit initial value is given, BayesFlux will draw it from an initialiser. Currently only one type of initialiser is implemented, but this can easily be extended by the user itself.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"init = InitialiseAllSame(Normal(0.0f0, 0.5f0), like, prior)  # First argument is dist we draw parameters from.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Given all the above, we can now define the BNN:","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"bnn = BNN(x, y, like, prior, init)","category":"page"},{"location":"introduction/linear-regression/#MAP-estimate.","page":"Example: Linear Regression","title":"MAP estimate.","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"It is always a good idea to first find the MAP estimate. This can serve two purposes:","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"It is faster than fully estimating the model using MCMC or VI and can thus serve as a quick check; If the MAP estimate results in bad point predictions, so will likely the full estimation results.\nIt can serve as a starting value for the MCMC samplers.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"To find a MAP estimate, we must first specify how we want to find it: We need to define an optimiser. BayesFlux currently only implements optimisers derived from Flux itself, but this can be extended by the user.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"opt = FluxModeFinder(bnn, Flux.ADAM())  # We will use ADAM\nθmap = find_mode(bnn, 10, 500, opt)  # batchsize 10 with 500 epochs","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"We can already use the MAP estimate to make some predictions and calculate the RMSE.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"nethat = nc(θmap)\nyhat = vec(nethat(x))\nsqrt(mean(abs2, y .- yhat))","category":"page"},{"location":"introduction/linear-regression/#MCMC-SGLD","page":"Example: Linear Regression","title":"MCMC - SGLD","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"If the MAP estimate does not show any problems, it can be used as the starting point for SGLD or any of the other MCMC methods (see later section).","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Simulations have shown that using a relatively large initial stepsize with a slow decaying stepsize schedule often results in the best mixing. Note: We would usually use samplers such as NUTS for linear regressions, which are much more efficient than SGLD","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"sampler = SGLD(Float32; stepsize_a = 10f-0, stepsize_b = 0.0f0, stepsize_γ = 0.55f0)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"We can obtain summary statistics and trace and density plots of network parameters and likelihood parameters by transforming the BayesFlux chain into a MCMCChain.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"using MCMCChains\nchain = Chains(ch')\nplot(chain)","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"In more complicated networks, it is usually a hopeless goal to obtain good mixing in parameter space and thus we rather focus on the output space of the network. Mixing in parameter space is hopeless due to the very complicated topology of the posterior; Simulations have also found that perfect mixing in the output space is not always needed to obtain good point and interval predictions. We will use a little helper function to get the output values of the network:","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"function naive_prediction(bnn, draws::Array{T, 2}; x = bnn.x, y = bnn.y) where {T}\n    yhats = Array{T, 2}(undef, length(y), size(draws, 2))\n    Threads.@threads for i=1:size(draws, 2)\n        net = bnn.like.nc(draws[:, i])\n        yh = vec(net(x))\n        yhats[:,i] = yh\n    end\n    return yhats\nend\n\nyhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Similarly, we can obtain posterior predictive values and evaluate quantiles obtained using these to how many percent of the actual data fall below the quantiles. What we would like is that 5% of the data fall below the 5% quantile of the posterior predictive draws.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"function get_observed_quantiles(y, posterior_yhat, target_q = 0.05:0.05:0.95)\n    qs = [quantile(yr, target_q) for yr in eachrow(posterior_yhat)]\n    qs = reduce(hcat, qs)\n    observed_q = mean(reshape(y, 1, :) .< qs; dims = 2)\n    return observed_q\nend\n\nposterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/#MCMC-SGNHTS","page":"Example: Linear Regression","title":"MCMC - SGNHTS","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Just like SGLD, SGNHTS also does not apply a Metropolis-Hastings correction step. Contrary to SGLD though, SGNHTS implementes a Thermostat, whose task it is to keep the temperature in the dynamic system close to one, and thus the sampling more accurate. Although the thermostats goal is often not achieved, samples obtained using SGNHTS often outperform those obtained using SGLD.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"sampler = SGNHTS(1f-2, 2f0; xi = 2f0^2, μ = 50f0)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"yhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/#MCMC-GGMC","page":"Example: Linear Regression","title":"MCMC - GGMC","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"As pointed out above, neither SGLD nor SGNHTS apply a Metropolis-Hastings acceptance step and are thus difficult to monitor. Indeed, draws from SGLD or SGNHTS should perhaps rather be considered as giving an ensemble of models rather than draws from the posterior, since without any MH step, it is unclear whether the chain actually will converge to the posterior.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"BayesFlux also implements three methods that do apply a MH step and are thus easier to monitor. These are GGMC, AdaptiveMH, and HMC. Both GGMC and HMC do allow for taking stochastic gradients. GGMC also allows to use delayed acceptance in which the MH step is only applied after a couple of steps, rather than after each step (see GGMC for details).","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Because both GGMC and HMC use a MH step, they provide a measure of the mean acceptance rate, which can be used to tune the stepsize using Dual Averaging (see DualAveragingStepSize details). Similarly, both also make use of mass matrices, which can also be tuned (see MassAdapter).","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"BayesFlux implements both stepsize adapters and mass adapters but to this point does not implement a smart way of combining them (this will come in the future). In my experience, naively combining them often only helps in more complex models and thus we will only use a stepsize adapter here.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"sadapter = DualAveragingStepSize(1f-9; target_accept = 0.55f0, adapt_steps = 10000)\nsampler = GGMC(Float32; β = 0.1f0, l = 1f-9, sadapter = sadapter)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"yhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"The above uses a MH correction after each step. This can be costly in big-data environments or when the evaluation of the likelihood is costly. If either of the above applies, delayed acceptance can speed up the process.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"sadapter = DualAveragingStepSize(1f-9; target_accept = 0.25f0, adapt_steps = 10000)\nsampler = GGMC(Float32; β = 0.1f0, l = 1f-9, sadapter = sadapter, steps = 3)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"yhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/#MCMC-HMC","page":"Example: Linear Regression","title":"MCMC - HMC","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"Since HMC showed some mixing problems for some variables during the testing of this README, we decided to use a mass matrix adaptation. This turned out to work better even in this simple case. Note that the use of stochastic gradients introduces problems into HMC and thus is not fully theoretically justified.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"sadapter = DualAveragingStepSize(1f-9; target_accept = 0.55f0, adapt_steps = 10000)\nmadapter = DiagCovMassAdapter(5000, 1000)\nsampler = HMC(1f-9, 5; sadapter = sadapter)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"yhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/#MCMC-Adaptive-Metropolis-Hastings","page":"Example: Linear Regression","title":"MCMC - Adaptive Metropolis-Hastings","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"As a derivative free alternative, BayesFlux also implements Adaptive MH as introduced (see AdaptiveMH). This is currently quite a costly method for complex models since it needs to evaluate the MH ratio at each step. Plans exist to parallelise the calculation of the likelihood which should speed up Adaptive MH.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"sampler = AdaptiveMH(diagm(ones(Float32, bnn.num_total_params)), 1000, 0.5f0, 1f-4)\nch = mcmc(bnn, 10, 50_000, sampler)\nch = ch[:, end-20_000+1:end]\nchain = Chains(ch')","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"yhats = naive_prediction(bnn, ch)\nchain_yhat = Chains(yhats')\nmaximum(summarystats(chain_yhat)[:, :rhat])","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"posterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"introduction/linear-regression/#Variation-Inference","page":"Example: Linear Regression","title":"Variation Inference","text":"","category":"section"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"In some cases MCMC method either do not work well or even the methods above take too long. For these cases BayesFlux currently implements Bayes-By-Backprop (see bbb); One shortcoming of the current implementation is that the variational family is constrained to a diagonal multivariate gaussian and thus any correlations between network parameters are set to zero. This can cause problems in some situations and plans exist to allow for more felxible covariance specifications.","category":"page"},{"location":"introduction/linear-regression/","page":"Example: Linear Regression","title":"Example: Linear Regression","text":"q, params, losses = bbb(bnn, 10, 2_000; mc_samples = 1, opt = Flux.ADAM(), n_samples_convergence = 10)\nch = rand(q, 20_000)\nposterior_yhat = sample_posterior_predict(bnn, ch)\nt_q = 0.05:0.05:0.95\no_q = get_observed_quantiles(y, posterior_yhat, t_q)\nplot(t_q, o_q, label = \"Posterior Predictive\", legend=:topleft,\n    xlab = \"Target Quantile\", ylab = \"Observed Quantile\")\nplot!(x->x, t_q, label = \"Target\")","category":"page"},{"location":"model/bnn/#Model-Basics","page":"Model Basics","title":"Model Basics","text":"","category":"section"},{"location":"model/bnn/","page":"Model Basics","title":"Model Basics","text":"BNN\nloglikeprior\n∇loglikeprior\nNetConstructor\ndestruct","category":"page"},{"location":"model/bnn/#BayesFlux.BNN","page":"Model Basics","title":"BayesFlux.BNN","text":"BNN(x, y, like::BNNLikelihood, prior::NetworkPrior, init::BNNInitialiser)\n\nCreate a Bayesian Neural Network. \n\nArguments\n\nx: Explanatory data\ny: Dependent variables\nlike: A likelihood\nprior: A prior on network parameters\ninit: An initilialiser\n\n\n\n\n\n","category":"type"},{"location":"model/bnn/#BayesFlux.loglikeprior","page":"Model Basics","title":"BayesFlux.loglikeprior","text":"Obtain the log of the unnormalised posterior.\n\n\n\n\n\n","category":"function"},{"location":"model/bnn/#BayesFlux.∇loglikeprior","page":"Model Basics","title":"BayesFlux.∇loglikeprior","text":"Obtain the derivative of the unnormalised log posterior.\n\n\n\n\n\n","category":"function"},{"location":"model/bnn/#BayesFlux.NetConstructor","page":"Model Basics","title":"BayesFlux.NetConstructor","text":"NetConstructor{T, F}\n\nUsed to construct a network from a vector.\n\nThe NetConstructor constains all important information to construct a network like the original network from a given vector. \n\nFields\n\nnum_params_net: Number of network parameters \nθ: Vector of network parameters of the original network\nstarts: Vector containing the starting points of each layer in θ\nends: Vector containing the end points of each layer in θ\nreconstructors: Vector containing the reconstruction functions for each layer\n\n\n\n\n\n","category":"type"},{"location":"model/bnn/#BayesFlux.destruct","page":"Model Basics","title":"BayesFlux.destruct","text":"destruct(net::Flux.Chain{T}) where {T}\n\nDestruct a network\n\nGiven a Flux.Chain network, destruct it and create a NetConstructor.  Each layer type must implement a destruct method taking the layer and returning a vector containing the original layer parameters, and a function that given a vector of the right length constructs a layer like the original using the parameters given in the vector\n\n\n\n\n\n","category":"function"},{"location":"likelihoods/interface/#Interface","page":"Interface","title":"Interface","text":"","category":"section"},{"location":"likelihoods/interface/","page":"Interface","title":"Interface","text":"BNNLikelihood","category":"page"},{"location":"likelihoods/interface/#BayesFlux.BNNLikelihood","page":"Interface","title":"BayesFlux.BNNLikelihood","text":"abstract type BNNLikelihood end\n\nEvery likelihood must be a subtype of BNNLikelihood and must implement at least the following fields: \n\nnum_params_like: The number of additional parameters introduced for which inference will be done (i.e. σ for a Gaussian but not ν for a T-Dist if df is not inferred)\nnc: A NetConstructor\n\nEvery BNNLikelihood must be callable in the following way \n\n(l::BNNLikelihood)(x, y, θnet, θlike)\n\nx either all data or a minibatch\ny either all data or a minibatch\nθnet are the network parameter\nθlike are the likelihood parameters. If no additional parameters were introduced, this will be an empty array\n\nEvery BNNLikelihood must also implement a posterior_predict method which should draw from the posterior predictive given network parameters and likelihood parameters. \n\nposterior_predict(l::BNNLikelihood, x, θnet, θlike)\n\nl the BNNLikelihood\nx the input data \nθnet network parameters\nθlike likelihood parameters\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#MCMC-Estimation","page":"MCMC Estimation","title":"MCMC Estimation","text":"","category":"section"},{"location":"inference/mcmc/#Sampler","page":"MCMC Estimation","title":"Sampler","text":"","category":"section"},{"location":"inference/mcmc/","page":"MCMC Estimation","title":"MCMC Estimation","text":"MCMCState\nmcmc\nSGLD\nSGNHTS\nSGNHT\nGGMC\nHMC\nAdaptiveMH","category":"page"},{"location":"inference/mcmc/#BayesFlux.MCMCState","page":"MCMC Estimation","title":"BayesFlux.MCMCState","text":"abstract type MCMCState end\n\nEvery MCMC method must be implemented via a MCMCState which keeps track of all  important information. \n\nMandatory Fields\n\nsamples::Matrix of dimension numtotalparameter×num_samples\nnsampled the number of thus far sampled samples\n\nMandatory Functions\n\nupdate!(sampler, θ, bnn, ∇θ) where θ is the current parameter vector and ∇θ(θ) is a function providing gradients. The function must return θ and num_samples so far sampled. \ninitialise!(sampler, θ, numsamples; continue_sampling) which initialises the sampler. If continue_sampling is true, then the final goal is to obtain numsamples samples and thus only the remaining ones still need to be sampled.\ncalculate_epochs(sampler, numbatches, numsamples; continue_sampling) which calculates the number of epochs that must be run through in order to obtain numsamples samples if numbatches batches are used. The number of epochs must be returned. If continue_sampling is true, then the goal is to obtain in total numsamples samples and thus we only need the number of epochs that still need to be run to obtain this total and NOT the number of epochs to sample numsamples new samples.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.mcmc","page":"MCMC Estimation","title":"BayesFlux.mcmc","text":"mcmc(args...; kwargs...)\n\nSample from a BNN using MCMC\n\nArguments\n\nbnn: a Bayesian Neural Network\nbatchsize: batchsize\nnumsamples: number of samples to take\nsampler: sampler to use \n\nKeyword Arguments\n\nshuffle::Bool=true: should data be shuffled after each epoch such that batches are different in each epoch? \npartial::Bool=true: are partial batches allowed? If true, some batches might be smaller than batchsize\nshowprogress::Bool=true: should a progress bar be shown? \ncontinue_sampling::Bool=false: If true and numsamples is larger than sampler.nsampled then additional samples will be taken \nθstart::AbstractVector{T}=vcat(bnn.init()...): starting parameter vector\n\n\n\n\n\n","category":"function"},{"location":"inference/mcmc/#BayesFlux.SGLD","page":"MCMC Estimation","title":"BayesFlux.SGLD","text":"Stochastic Gradient Langevin Dynamics as proposed in Welling, M., & Teh, Y. W. (n.d.). Bayesian Learning via Stochastic Gradient Langevin Dynamics. 8.\n\nFields\n\nθ::AbstractVector: Current sample\nsamples::Matrix: Matrix of samples. Not all columns will be actual samples if sampling was stopped early. See nsampled for the actual number of samples taken. \nnsampled::Int: Number of samples taken\nmin_stepsize::T: Stop decreasing the stepsize when it is below this value. \ndidinform::Bool: Flag keeping track of whether we informed user that min_stepsize was reached. \nstepsize_a::T: See stepsize\nstepsize_b::T: See stepsize\nstepsize_γ::T: See stepsize\nmaxnorm::T: Maximimum gradient norm. Gradients are being clipped if norm exceeds this value\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.SGNHTS","page":"MCMC Estimation","title":"BayesFlux.SGNHTS","text":"Stochastic Gradient Nose-Hoover Thermostat as proposed in \n\nProposed in Leimkuhler, B., & Shang, X. (2016). Adaptive thermostats for noisy gradient systems. SIAM Journal on Scientific Computing, 38(2), A712-A736.\n\nThis is similar to SGNHT as proposed in  Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., & Neven, H. (2014). Bayesian sampling using stochastic gradient thermostats. Advances in neural information processing systems, 27.\n\nFields\n\nsamples::Matrix: Containing the samples \nnsampled::Int: Number of samples taken so far. Can be smaller than size(samples, 2) if sampling was interrupted. \np::AbstractVector: Momentum\nxi::Number: Thermostat\nl::Number: Stepsize; This often is in the 0.001-0.1 range.\nσA::Number: Diffusion factor; If the stepsize is small, this should be larger than 1.\nμ::Number: Free parameter in thermostat. Defaults to 1.\nt::Int: Current step count\nkinetic::Vector: Keeps track of the kinetic energy. Goal of SGNHT is to have the average close to one\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.SGNHT","page":"MCMC Estimation","title":"BayesFlux.SGNHT","text":"Stochastic Gradient Nose-Hoover Thermostat as proposed in \n\nDing, N., Fang, Y., Babbush, R., Chen, C., Skeel, R. D., & Neven, H. (2014). Bayesian sampling using stochastic gradient thermostats. Advances in neural information processing systems, 27.\n\nFields\n\nsamples::Matrix: Containing the samples \nnsampled::Int: Number of samples taken so far. Can be smaller than size(samples, 2) if sampling was interrupted. \np::AbstractVector: Momentum\nxi::Number: Thermostat\nl::Number: Stepsize\nA::Number: Diffusion factor\nt::Int: Current step count\nkinetic::Vector: Keeps track of the kinetic energy. Goal of SGNHT is to have the average close to one\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.GGMC","page":"MCMC Estimation","title":"BayesFlux.GGMC","text":"Gradient Guided Monte Carlo\n\nProposed in Garriga-Alonso, A., & Fortuin, V. (2021). Exact langevin dynamics with stochastic gradients. arXiv preprint arXiv:2102.01691.\n\nFields\n\nsamples::Matrix: Matrix containing the samples. If sampling stopped early, then not all columns will actually correspond to samples. See nsampled to check how many samples were actually taken\nnsampled::Int: Number of samples taken. \nt::Int: Total number of steps taken.\naccepted::Vector{Bool}: If true, sample was accepted; If false, proposed sample was rejected and previous sample was taken. \nβ::T: See paper. \nl::T: Step-length; See paper.\nsadapter::StepsizeAdapter: A StepsizeAdapter. Default is DualAveragingStepSize\nM::AbstractMatrix: Mass Matrix\nMhalf::AbstractMatrix: Lower triangual cholesky decomposition of M\nMinv::AbstractMatrix: Inverse mass matrix.\nmadapter::MassAdapter: A MassAdapter\nmomentum::AbstractVector: Last momentum vector\nlMH::T: log of Metropolis-Hastings ratio. \nsteps::Int: Number of steps to take before calculating MH ratio. \ncurrent_step::Int: Current step in the recurrent sequence 1, ..., steps. \nmaxnorm::T: Maximimum gradient norm. Gradients are being clipped if norm exceeds this value\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.HMC","page":"MCMC Estimation","title":"BayesFlux.HMC","text":"Standard Hamiltonian Monte Carlo (Hybrid Monte Carlo).\n\nAllows for the use of stochastic gradients, but the validity of doing so is not clear. \n\nThis is motivated by parts of the discussion in  Neal, R. M. (1996). Bayesian Learning for Neural Networks (Vol. 118). Springer New York. https://doi.org/10.1007/978-1-4612-0745-0\n\nCode was partially adapted from https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/\n\nFields\n\nsamples::Matrix: Samples taken\nnsampled::Int: Number of samples taken. Might be smaller than size(samples) if sampling was interrupted.\nθold::AbstractVector: Old sample. Kept for rejection step.\nmomentum::AbstractVector: Momentum variables\nmomentumold::AbstractVector: Old momentum variables. Kept for rejection step.\nt::Int: Current step.\npath_len::Int: Number of leapfrog steps. \ncurrent_step::Int: Current leapfrog step.\naccepted::Vector{Bool}: Whether a draw in samples was a accepted draw or rejected (in which case it is the same as the previous one.)\nsadapter::StepsizeAdapter: Stepsize adapter giving the stepsize in each iteration.\nl: Stepsize.\nmadapter::MassAdapter: Mass matrix adapter giving the inverse mass matrix in each iteration.\nMinv::AbstractMatrix: Inverse mass matrix\nmaxnorm::T: Maximimum gradient norm. Gradients are being clipped if norm exceeds this value\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.AdaptiveMH","page":"MCMC Estimation","title":"BayesFlux.AdaptiveMH","text":"Adaptive Metropolis Hastings as introduced in \n\nHaario, H., Saksman, E., & Tamminen, J. (2001). An adaptive Metropolis algorithm. Bernoulli, 223-242.\n\nFields\n\nsamples::Matix: Matrix holding the samples. If sampling was stopped early, not all columns will represent samples. To figure out how many columns represent samples, check out nsampled.\nnsampled::Int: Number of samples obtained.\nC0::Matrix: Initial covariance matrix. \nCt::Matrix: Covariance matrix in iteration t \nt::Int: Current time period\nt0::Int: When to start adaptig the covariance matrix? Covariance is adapted in a rolling window form. \nsd::T: See the paper. \nϵ::T: Will be added to diagonal to prevent numerical non-pod-def problems. If you run into numerical problems, try increasing this values.\naccepted::Vector{Bool}: For each sample, indicating whether the sample was accepted (true) or the previous samples was chosen (false)\n\nNotes\n\nAdaptive MH might not be suited if it is very costly to calculate the likelihood as this needs to be done for each sample on the full dataset. Plans exist to make this faster. \nWorks best when started at a MAP estimate. \n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#Mass-Adaptation","page":"MCMC Estimation","title":"Mass Adaptation","text":"","category":"section"},{"location":"inference/mcmc/","page":"MCMC Estimation","title":"MCMC Estimation","text":"MassAdapter\nDiagCovMassAdapter\nFullCovMassAdapter\nFixedMassAdapter\nRMSPropMassAdapter","category":"page"},{"location":"inference/mcmc/#BayesFlux.MassAdapter","page":"MCMC Estimation","title":"BayesFlux.MassAdapter","text":"Adapt the mass matrix in MCMC and especially dynamic MCMC methods such as   HMC, GGMC, SGLD, SGNHT, ...\n\nMandatory Fields\n\nMinv::AbstractMatrix: The inverse mass matrix used in HMC, GGMC, ...\n\nMandatory Functions\n\n(madapter::MassAdapter)(s::MCMCState, θ::AbstractVector, bnn, ∇θ): Every mass adapter must be callable and have the sampler state, the current sample, the BNN and a gradient function as arguments. It must return the new Minv Matrix.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.DiagCovMassAdapter","page":"MCMC Estimation","title":"BayesFlux.DiagCovMassAdapter","text":"Use the variances as the diagonal of the inverse mass matrix as used in HMC,  GGMC, ...; \n\nFields\n\nMinv: Inverse mass matrix as used in HMC, SGLD, GGMC, ...\nadapt_steps: Number of adaptation steps. \nwindowlength: Lookback length for calculation of covariance.\nt: Current step. \nkappa: How much to shrink towards the identity.\nepsilon: Small value to add to diagonal to avoid numerical instability.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.FullCovMassAdapter","page":"MCMC Estimation","title":"BayesFlux.FullCovMassAdapter","text":"Use the full covariance matrix of a moving average of samples as the mass matrix. This is similar to what is already done in Adaptive MH.\n\nFields\n\nMinv: Inverse mass matrix as used in HMC, SGLD, GGMC, ...\nadapt_steps: Number of adaptation steps. \nwindowlength: Lookback length for calculation of covariance.\nt: Current step.\nkappa: How much to shrink towards the identity.\nepsilon: Small value to add to diagonal to avoid numerical instability.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.FixedMassAdapter","page":"MCMC Estimation","title":"BayesFlux.FixedMassAdapter","text":"Use a fixed inverse mass matrix.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.RMSPropMassAdapter","page":"MCMC Estimation","title":"BayesFlux.RMSPropMassAdapter","text":"Use RMSProp as a precondition/mass matrix adapter. This was proposed in \n\nLi, C., Chen, C., Carlson, D., & Carin, L. (2016, February). Preconditioned stochastic gradient Langevin dynamics for deep neural networks. In Thirtieth AAAI Conference on Artificial Intelligence for the use in SGLD and related methods. \n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#Stepsize-Adaptation","page":"MCMC Estimation","title":"Stepsize Adaptation","text":"","category":"section"},{"location":"inference/mcmc/","page":"MCMC Estimation","title":"MCMC Estimation","text":"StepsizeAdapter\nConstantStepsize\nDualAveragingStepSize","category":"page"},{"location":"inference/mcmc/#BayesFlux.StepsizeAdapter","page":"MCMC Estimation","title":"BayesFlux.StepsizeAdapter","text":"Adapt the stepsize of MCMC algorithms.\n\nImplentation Details\n\nMandatory Fields\n\nl::Number The stepsize. Will be used by the sampler.\n\nMandatory Functions\n\n(sadapter::StepsizeAdapter)(s::MCMCState, mh_probability::Number) Every stepsize adapter must be callable with arguments, being the sampler itself and the Metropolis-Hastings acceptance probability. The method must return the new stepsize.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.ConstantStepsize","page":"MCMC Estimation","title":"BayesFlux.ConstantStepsize","text":"Use a contant stepsize.\n\n\n\n\n\n","category":"type"},{"location":"inference/mcmc/#BayesFlux.DualAveragingStepSize","page":"MCMC Estimation","title":"BayesFlux.DualAveragingStepSize","text":"Use the Dual Average method to tune the stepsize.\n\nThe use of the Dual Average method was proposed in:\n\nHoffman, M. D., & Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15, 31.\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/feedforward/#Feedforward-Likelihoods","page":"Feedforward Likelihoods","title":"Feedforward Likelihoods","text":"","category":"section"},{"location":"likelihoods/feedforward/","page":"Feedforward Likelihoods","title":"Feedforward Likelihoods","text":"FeedforwardNormal\nFeedforwardTDist","category":"page"},{"location":"likelihoods/feedforward/#BayesFlux.FeedforwardNormal","page":"Feedforward Likelihoods","title":"BayesFlux.FeedforwardNormal","text":"FeedforwardNormal(nc::NetConstructor{T, F}, prior_σ::D) where {T, F, D<:Distribution}\n\nUse a Gaussian/Normal likelihood for a Feedforward architecture with a single output.\n\nAssumes is a single output. Thus, the last layer must have output size one. \n\nArguments\n\nnc: obtained using destruct\nprior_σ: a prior distribution for the standard deviation\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/feedforward/#BayesFlux.FeedforwardTDist","page":"Feedforward Likelihoods","title":"BayesFlux.FeedforwardTDist","text":"FeedforwardTDist(nc::NetConstructor{T, F}, prior_σ::D, ν::T) where {T, F, D}\n\nUse a Student-T likelihood for a Feedforward architecture with a single output and known degress of freedom.\n\nAssumes is a single output. Thus, the last layer must have output size one. \n\nArguments\n\nnc: obtained using destruct\nprior_σ: a prior distribution for the standard deviation\nν: degrees of freedom\n\n\n\n\n\n","category":"type"},{"location":"inference/vi/#Variational-Inference","page":"Variational Inference","title":"Variational Inference","text":"","category":"section"},{"location":"inference/vi/","page":"Variational Inference","title":"Variational Inference","text":"bbb","category":"page"},{"location":"inference/vi/#BayesFlux.bbb","page":"Variational Inference","title":"BayesFlux.bbb","text":"bbb(args...; kwargs...)\n\nUse Bayes By Backprop to find Variational Approximation to BNN. \n\nThis was proposed in Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015, June). Weight uncertainty in neural network. In International conference on machine learning (pp. 1613-1622). PMLR.\n\nArguments\n\nbnn::BNN: The Bayesian NN \nbatchsize::Int: Batchsize \nepochs::Int: Epochs \n\nKeyword Arguments\n\nmc_samples::Int=1: Over how many gradients should be averaged?\nshuffle::Bool=true: Should observations be shuffled after each epoch?\npartial::Bool=true: Can the last batch be smaller than batchsize? \nshowprogress::Bool=true: Show progress bar? \nopt=Flux.ADAM(): Must be an optimiser of type Flux.Optimiser \nn_samples_convergence::Int=10: After each epoch the loss is calculated and kept track of using an average of n_samples_convergence samples. \n\n\n\n\n\n","category":"function"},{"location":"utils/recurrent/#Recurrent-Architectures","page":"Recurrent Architectures","title":"Recurrent Architectures","text":"","category":"section"},{"location":"utils/recurrent/","page":"Recurrent Architectures","title":"Recurrent Architectures","text":"make_rnn_tensor","category":"page"},{"location":"utils/recurrent/#BayesFlux.make_rnn_tensor","page":"Recurrent Architectures","title":"BayesFlux.make_rnn_tensor","text":"make_rnn_tensor(m::Matrix{T}, seq_to_one_length = 10) where {T}\n\nCreate a Tensor used for RNNs \n\nGiven an input matrix of dimensions timesteps×features transform it into a Tensor of dimension timesteps×features×sequences where the sequences are overlapping subsequences of length seq_to_one_length of the orignal timesteps long sequence\n\n\n\n\n\n","category":"function"},{"location":"inference/map/#MAP-Estimation","page":"MAP Estimation","title":"MAP Estimation","text":"","category":"section"},{"location":"inference/map/","page":"MAP Estimation","title":"MAP Estimation","text":"BNNModeFinder\nfind_mode\nFluxModeFinder","category":"page"},{"location":"inference/map/#BayesFlux.BNNModeFinder","page":"MAP Estimation","title":"BayesFlux.BNNModeFinder","text":"Find the mode of a BNN. \n\nFind the mode of a BNN using optimiser. Each optimiser must have implemented  a function step!(optimiser, θ, ∇θ) which makes one optimisation step given  gradient function ∇θ(θ) and current parameter vector θ. The function must return  θ as the first return value and a flag has_converged indicating whether the  optimisation procedure should be stopped. \n\n\n\n\n\n","category":"type"},{"location":"inference/map/#BayesFlux.find_mode","page":"MAP Estimation","title":"BayesFlux.find_mode","text":"find_mode(bnn::BNN, batchsize::Int, epochs::Int, optimiser::BNNModeFinder)\n\nFind the mode of a BNN.\n\nArguments\n\nbnn::BNN: A Bayesian Neural Network formed using BNN. \nbatchsize::Int: Batchsize used for stochastic gradients. \nepochs::Int: Number of epochs to run for.\noptimiser::BNNModeFinder: An optimiser.\n\nKeyword Arguments\n\nshuffle::Bool=true: Should data be shuffled after each epoch?\npartial::Bool=true: Is it allowed to use a batch that is smaller than batchsize?\nshowprogress::Bool=true: Show a progress bar? \n\n\n\n\n\n","category":"function"},{"location":"inference/map/#BayesFlux.FluxModeFinder","page":"MAP Estimation","title":"BayesFlux.FluxModeFinder","text":"FluxModeFinder(bnn::BNN, opt::O; windowlength = 100, ϵ = 1e-6) where {O<:Flux.Optimise.AbstractOptimiser}\n\nUse one of Flux optimisers to find the mode. Keep track of changes in θ over a window  of windowlegnth and report convergence if the maximum change over the current window is  smaller than ϵ.\n\n\n\n\n\n","category":"type"},{"location":"priors/interface/#Interface","page":"Interface","title":"Interface","text":"","category":"section"},{"location":"priors/interface/","page":"Interface","title":"Interface","text":"NetworkPrior","category":"page"},{"location":"priors/interface/#BayesFlux.NetworkPrior","page":"Interface","title":"BayesFlux.NetworkPrior","text":"abstract type NetworkPrior end\n\nAll priors over the network parameters are subtypes of NetworkPrior. \n\nRequired Fields\n\nnum_params_hyper: Number of hyper priors (only those that should be inferred)\nnc::NetConstructor: NetConstructor\n\nRequired implementations\n\nEach NetworkPrior must be callable in the form of \n\n(np::NetworkPrior)(θnet, θhyper)\n\nθnet is a vector of network parameters\nθhyper is a vector of hyper parameters\n\nThe return value must be the logprior including all hyper-priors. \n\nEach NetworkPrior must also implement a sample function returning a vector θnet of network parameters drawn from the prior. \n\nsample_prior(np::NetworkPrior)(rng::AbstractRNG)\n\n\n\n\n\n","category":"type"},{"location":"priors/mixturescale/#Mixture-Scale-Prior","page":"Mixture Scale Prior","title":"Mixture Scale Prior","text":"","category":"section"},{"location":"priors/mixturescale/","page":"Mixture Scale Prior","title":"Mixture Scale Prior","text":"MixtureScalePrior","category":"page"},{"location":"priors/mixturescale/#BayesFlux.MixtureScalePrior","page":"Mixture Scale Prior","title":"BayesFlux.MixtureScalePrior","text":"MixtureScalePrior(nc::NetConstructor{T, F}, σ1::T, σ2::T, μ1::T)\n\nScale mixture of Gaussians\n\nFields\n\nnum_params_hyper::Int=0: Number of hyper priors\nnc::NetConstructor: NetConstructor object\nσ1: Standard deviation of first Gaussian \nσ2: Standard deviation of second Gaussian \nπ1: Weight/Probability of first Gaussian \nπ2: Weight/Probability of second Gaussian (1-π1)\n\n\n\n\n\n","category":"type"},{"location":"#BayesFlux.jl-a-Bayesian-extension-to-[Flux.jl](https://fluxml.ai)","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"","category":"section"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"using BayesFlux, Flux\nusing Random, Distributions\nusing StatsPlots\nusing LinearAlgebra\n\nRandom.seed!(6150533)","category":"page"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"CurrentModule = BayesFlux\nDocTestSetup = quote\n    using BayesFlux\nend","category":"page"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"BayesFlux is meant to be an extension to Flux.jl, a machine learning library written entirely in Julia. BayesFlux will and is not meant to be the fastest production ready library, but rather is meant to make research and experimentation easy.","category":"page"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"BayesFlux is part of my Master Thesis in Economic and Financial Research - specialisation Econometrics at Maastricht University and will therefore likely still go through some revisions in the coming months.","category":"page"},{"location":"#Structure","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"Structure","text":"","category":"section"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"Every Bayesian model can in general be broken down into the probabilistic model, which gives the likelihood function and the prior on all parameters of the probabilistic model. BayesFlux somewhat follows this and splits every Bayesian Network into the following parts:","category":"page"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"Network: Every BNN must have some general network structure. This is defined using Flux and currently supports Dense, RNN, and LSTM layers. More on this later\nNetwork Constructor: Since BayesFlux works with vectors of parameters, we need to be able to go from a vector to the network and back. This works by using the NetworkConstructor.\nLikelihood: The likelihood function. In traditional estimation of NNs, this would correspond to the negative loss function. BayesFlux has a twist on this though and nomenclature might change because of this twist: The likelihood also contains all additional parameters and priors. For example, for a Gaussian likelihood, the likelihood object also defines the standard deviation and the prior for the standard deviation. This design choice was made to keep the likelihood and everything belonging to it separate from the network; Again, due to the potential confusion, the nomenclature might change in later revisions.\nPrior on network parameters: A prior on all network parameters. Currently the RNN layers do not define priors on the initial state and thus the initial state is also not sampled. Priors can have hyper-priors.\nInitialiser: Unless some special initialisation values are given, BayesFlux will draw initial values as defined by the initialiser. An initialiser initialises all network and likelihood parameters to reasonable values.","category":"page"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"All the above are then used to create a BNN which can then be estimated using the MAP, can be sampled from using any of the MCMC methods implemented, or can be estimated using Variational Inference.","category":"page"},{"location":"","page":"BayesFlux.jl a Bayesian extension to Flux.jl","title":"BayesFlux.jl a Bayesian extension to Flux.jl","text":"The examples and the sections below hopefully clarify everything. If any questions remain, please open an issue.","category":"page"},{"location":"initialise/init/#Initialisation","page":"Initialisation","title":"Initialisation","text":"","category":"section"},{"location":"initialise/init/","page":"Initialisation","title":"Initialisation","text":"BNNInitialiser\nInitialiseAllSame","category":"page"},{"location":"initialise/init/#BayesFlux.BNNInitialiser","page":"Initialisation","title":"BayesFlux.BNNInitialiser","text":"abstract type BNNInitialiser end\n\nTo initialise BNNs, BNNInitialisers are used. These must be callable and must return three vectors \n\nθnet the initial values for the network parameters\nθhyper the initial values for any hyperparameters introduced by the NetworkPrior\nθlike the initial values for any extra parameters introduced by the likelihood \n\nImplementation\n\nEvery BNNInitialiser must be callable and must return the above three things: \n\n(init::BNNInitialiser)(rng::AbstractRNG) -> (θnet, θhyper, θlike)\n\n\n\n\n\n","category":"type"},{"location":"initialise/init/#BayesFlux.InitialiseAllSame","page":"Initialisation","title":"BayesFlux.InitialiseAllSame","text":"InitialiseAllSame(dist::D, like::BNNLikelihood, prior::NetworkPrior)\n\nInitialise all values by drawing from dist\n\n\n\n\n\n","category":"type"}]
}

<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example: Linear Regression · BFlux.jl Documentation</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BFlux.jl Documentation</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../">BFlux.jl a Bayesian extension to Flux.jl</a></li><li class="is-active"><a class="tocitem" href>Example: Linear Regression</a></li><li><a class="tocitem" href="../feedforward/">Example: Feedforward NN Regression</a></li><li><a class="tocitem" href="../recurrent/">Example: Recurrent Neural Networks</a></li></ul></li><li><span class="tocitem">Bayesian Neural Networks</span><ul><li><a class="tocitem" href="../../model/bnn/">Model Basics</a></li><li><a class="tocitem" href="../../model/sampling/">Prior and Posterior Predictive</a></li></ul></li><li><span class="tocitem">Likelihood Functions</span><ul><li><a class="tocitem" href="../../likelihoods/interface/">Interface</a></li><li><a class="tocitem" href="../../likelihoods/feedforward/">Feedforward Likelihoods</a></li><li><a class="tocitem" href="../../likelihoods/seqtoone/">Seq-to-One Likelihoods</a></li></ul></li><li><span class="tocitem">Network Priors</span><ul><li><a class="tocitem" href="../../priors/interface/">Interface</a></li><li><a class="tocitem" href="../../priors/gaussian/">Gaussian Prior</a></li><li><a class="tocitem" href="../../priors/mixturescale/">Mixture Scale Prior</a></li></ul></li><li><span class="tocitem">Inference</span><ul><li><a class="tocitem" href="../../inference/map/">MAP Estimation</a></li><li><a class="tocitem" href="../../inference/mcmc/">MCMC Estimation</a></li><li><a class="tocitem" href="../../inference/vi/">Variational Inference</a></li></ul></li><li><span class="tocitem">Initialisation</span><ul><li><a class="tocitem" href="../../initialise/init/">Initialisation</a></li></ul></li><li><span class="tocitem">Utils</span><ul><li><a class="tocitem" href="../../utils/recurrent/">Recurrent Architectures</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Introduction</a></li><li class="is-active"><a href>Example: Linear Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example: Linear Regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/enweg/BFlux.jl/blob/main/docs/src/introduction/linear-regression.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Example:-Linear-Regression"><a class="docs-heading-anchor" href="#Example:-Linear-Regression">Example: Linear Regression</a><a id="Example:-Linear-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Linear-Regression" title="Permalink"></a></h1><p>Although not meant for Simple Linear Regression, BFlux can be used for it, and we will do so in this section. This will hopefully demonstrate the basics. Later sections will show better examples.</p><p>Let&#39;s say we have the idea that the data can be modelled via a linear model of the form <span>$y_i = x_i&#39;\beta + e_i$</span> with <span>$e_i \sim N(0, 1)$</span></p><pre><code class="language-julia hljs">k = 5
n = 500
x = randn(Float32, k, n);
β = randn(Float32, k);
y = x&#39;*β + randn(Float32, n);</code></pre><p>This is a standard linear model and we would likely be better off using STAN or Turing for this, but due to the availability of a Dense layer with linear activation function, we can also implement it in BFlux.</p><p>The first step is to define the network. As mentioned above, the network consists of a single Dense layer with a linear activation function (the default activation in Flux and hence not explicitly shown).</p><pre><code class="language-julia hljs">net = Chain(Dense(k, 1))  # k inputs and one output</code></pre><p>Since BFlux works with vectors, we need to be able to transform a vector to the above network and back. We thus need a NetworkConstructor, which we obtain as a the return value of a <code>destruct</code></p><pre><code class="language-julia hljs">nc = destruct(net)</code></pre><p>We can check whether everything works by just creating a random vector of the right dimension and calling the NetworkConstructor using this vector.</p><pre><code class="language-julia hljs">θ = randn(Float32, nc.num_params_network)
nc(θ)</code></pre><p>We indeed obtain a network of the right size and structure. Next, we will define a prior for all parameters of the network. Since weight decay is a popular regularisation method in standard ML estimation, we will be using a Gaussian prior, which is the Bayesian weight decay:</p><pre><code class="language-julia hljs">prior = GaussianPrior(nc, 0.5f0)  # the last value is the standard deviation</code></pre><p>We also need a likelihood and a prior on all parameters the likelihood introduces to the model. We will go for a Gaussian likelihood, which introduces the standard deviation of the model. BFlux currently implements Gaussian and Student-t likelihoods for Feedforward and Seq-to-one cases but more can easily be implemented.</p><pre><code class="language-julia hljs">like = FeedforwardNormal(nc, Gamma(2.0, 0.5))  # Second argument is prior for standard deviation.</code></pre><p>Lastly, when no explicit initial value is given, BFlux will draw it from an initialiser. Currently only one type of initialiser is implemented, but this can easily be extended by the user itself.</p><pre><code class="language-julia hljs">init = InitialiseAllSame(Normal(0.0f0, 0.5f0), like, prior)  # First argument is dist we draw parameters from.</code></pre><p>Given all the above, we can now define the BNN:</p><pre><code class="language-julia hljs">bnn = BNN(x, y, like, prior, init)</code></pre><h3 id="MAP-estimate."><a class="docs-heading-anchor" href="#MAP-estimate.">MAP estimate.</a><a id="MAP-estimate.-1"></a><a class="docs-heading-anchor-permalink" href="#MAP-estimate." title="Permalink"></a></h3><p>It is always a good idea to first find the MAP estimate. This can serve two purposes:</p><ol><li>It is faster than fully estimating the model using MCMC or VI and can thus serve as a quick check; If the MAP estimate results in bad point predictions, so will likely the full estimation results.</li><li>It can serve as a starting value for the MCMC samplers.</li></ol><p>To find a MAP estimate, we must first specify how we want to find it: We need to define an optimiser. BFlux currently only implements optimisers derived from Flux itself, but this can be extended by the user.</p><pre><code class="language-julia hljs">opt = FluxModeFinder(bnn, Flux.ADAM())  # We will use ADAM
θmap = find_mode(bnn, 10, 500, opt)  # batchsize 10 with 500 epochs</code></pre><p>We can already use the MAP estimate to make some predictions and calculate the RMSE.</p><pre><code class="language-julia hljs">nethat = nc(θmap)
yhat = vec(nethat(x))
sqrt(mean(abs2, y .- yhat))</code></pre><h3 id="MCMC-SGLD"><a class="docs-heading-anchor" href="#MCMC-SGLD">MCMC - SGLD</a><a id="MCMC-SGLD-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-SGLD" title="Permalink"></a></h3><p>If the MAP estimate does not show any problems, it can be used as the starting point for SGLD or any of the other MCMC methods (see later section).</p><p>Simulations have shown that using a relatively large initial stepsize with a slow decaying stepsize schedule often results in the best mixing. <em>Note: We would usually use samplers such as NUTS for linear regressions, which are much more efficient than SGLD</em></p><pre><code class="language-julia hljs">sampler = SGLD(Float32; stepsize_a = 10f-0, stepsize_b = 0.0f0, stepsize_γ = 0.55f0)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]</code></pre><p>We can obtain summary statistics and trace and density plots of network parameters and likelihood parameters by transforming the BFlux chain into a MCMCChain.</p><pre><code class="language-julia hljs">using MCMCChains
chain = Chains(ch&#39;)
plot(chain)</code></pre><p>In more complicated networks, it is usually a hopeless goal to obtain good mixing in parameter space and thus we rather focus on the output space of the network. <em>Mixing in parameter space is hopeless due to the very complicated topology of the posterior; Simulations have also found that perfect mixing in the output space is not always needed to obtain good point and interval predictions.</em> We will use a little helper function to get the output values of the network:</p><pre><code class="language-julia hljs">function naive_prediction(bnn, draws::Array{T, 2}; x = bnn.x, y = bnn.y) where {T}
    yhats = Array{T, 2}(undef, length(y), size(draws, 2))
    Threads.@threads for i=1:size(draws, 2)
        net = bnn.like.nc(draws[:, i])
        yh = vec(net(x))
        yhats[:,i] = yh
    end
    return yhats
end

yhats = naive_prediction(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><p>Similarly, we can obtain posterior predictive values and evaluate quantiles obtained using these to how many percent of the actual data fall below the quantiles. What we would like is that 5% of the data fall below the 5% quantile of the posterior predictive draws.</p><pre><code class="language-julia hljs">function get_observed_quantiles(y, posterior_yhat, target_q = 0.05:0.05:0.95)
    qs = [quantile(yr, target_q) for yr in eachrow(posterior_yhat)]
    qs = reduce(hcat, qs)
    observed_q = mean(reshape(y, 1, :) .&lt; qs; dims = 2)
    return observed_q
end

posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre><h3 id="MCMC-SGNHTS"><a class="docs-heading-anchor" href="#MCMC-SGNHTS">MCMC - SGNHTS</a><a id="MCMC-SGNHTS-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-SGNHTS" title="Permalink"></a></h3><p>Just like SGLD, SGNHTS also does not apply a Metropolis-Hastings correction step. Contrary to SGLD though, SGNHTS implementes a Thermostat, whose task it is to keep the temperature in the dynamic system close to one, and thus the sampling more accurate. Although the thermostats goal is often not achieved, samples obtained using SGNHTS often outperform those obtained using SGLD.</p><pre><code class="language-julia hljs">sampler = SGNHTS(1f-2, 2f0; xi = 2f0^2, μ = 50f0)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]
chain = Chains(ch&#39;)</code></pre><hr/><pre><code class="language-julia hljs">yhats = naive_prediction(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><hr/><pre><code class="language-julia hljs">posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre><h3 id="MCMC-GGMC"><a class="docs-heading-anchor" href="#MCMC-GGMC">MCMC - GGMC</a><a id="MCMC-GGMC-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-GGMC" title="Permalink"></a></h3><p>As pointed out above, neither SGLD nor SGNHTS apply a Metropolis-Hastings acceptance step and are thus difficult to monitor. Indeed, draws from SGLD or SGNHTS should perhaps rather be considered as giving an ensemble of models rather than draws from the posterior, since without any MH step, it is unclear whether the chain actually will converge to the posterior.</p><p>BFlux also implements three methods that do apply a MH step and are thus easier to monitor. These are GGMC, AdaptiveMH, and HMC. Both GGMC and HMC do allow for taking stochastic gradients. GGMC also allows to use delayed acceptance in which the MH step is only applied after a couple of steps, rather than after each step (see <a href="../../inference/mcmc/#BFlux.GGMC"><code>GGMC</code></a> for details).</p><p>Because both GGMC and HMC use a MH step, they provide a measure of the mean acceptance rate, which can be used to tune the stepsize using Dual Averaging (see <a href="../../inference/mcmc/#BFlux.DualAveragingStepSize"><code>DualAveragingStepSize</code></a> details). Similarly, both also make use of mass matrices, which can also be tuned (see <a href="../../inference/mcmc/#BFlux.MassAdapter"><code>MassAdapter</code></a>).</p><p>BFlux implements both stepsize adapters and mass adapters but to this point does not implement a smart way of combining them (this will come in the future). In my experience, naively combining them often only helps in more complex models and thus we will only use a stepsize adapter here.</p><pre><code class="language-julia hljs">sadapter = DualAveragingStepSize(1f-9; target_accept = 0.55f0, adapt_steps = 10000)
sampler = GGMC(Float32; β = 0.1f0, l = 1f-9, sadapter = sadapter)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]
chain = Chains(ch&#39;)</code></pre><hr/><pre><code class="language-julia hljs">yhats = naive_prediction(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><hr/><pre><code class="language-julia hljs">posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre><p>The above uses a MH correction after each step. This can be costly in big-data environments or when the evaluation of the likelihood is costly. If either of the above applies, delayed acceptance can speed up the process.</p><pre><code class="language-julia hljs">sadapter = DualAveragingStepSize(1f-9; target_accept = 0.25f0, adapt_steps = 10000)
sampler = GGMC(Float32; β = 0.1f0, l = 1f-9, sadapter = sadapter, steps = 3)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]
chain = Chains(ch&#39;)</code></pre><hr/><pre><code class="language-julia hljs">yhats = naive_prediction(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><hr/><pre><code class="language-julia hljs">posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre><h3 id="MCMC-HMC"><a class="docs-heading-anchor" href="#MCMC-HMC">MCMC - HMC</a><a id="MCMC-HMC-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-HMC" title="Permalink"></a></h3><p>Since HMC showed some mixing problems for some variables during the testing of this README, we decided to use a mass matrix adaptation. This turned out to work better even in this simple case. Note that the use of stochastic gradients introduces problems into HMC and thus is not fully theoretically justified.</p><pre><code class="language-julia hljs">sadapter = DualAveragingStepSize(1f-9; target_accept = 0.55f0, adapt_steps = 10000)
madapter = DiagCovMassAdapter(5000, 1000)
sampler = HMC(1f-9, 5; sadapter = sadapter)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]
chain = Chains(ch&#39;)</code></pre><hr/><pre><code class="language-julia hljs">yhats = naive_prediction(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><hr/><pre><code class="language-julia hljs">posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre><h3 id="MCMC-Adaptive-Metropolis-Hastings"><a class="docs-heading-anchor" href="#MCMC-Adaptive-Metropolis-Hastings">MCMC - Adaptive Metropolis-Hastings</a><a id="MCMC-Adaptive-Metropolis-Hastings-1"></a><a class="docs-heading-anchor-permalink" href="#MCMC-Adaptive-Metropolis-Hastings" title="Permalink"></a></h3><p>As a derivative free alternative, BFlux also implements Adaptive MH as introduced (see <a href="../../inference/mcmc/#BFlux.AdaptiveMH"><code>AdaptiveMH</code></a>). This is currently quite a costly method for complex models since it needs to evaluate the MH ratio at each step. Plans exist to parallelise the calculation of the likelihood which should speed up Adaptive MH.</p><pre><code class="language-julia hljs">sampler = AdaptiveMH(diagm(ones(Float32, bnn.num_total_params)), 1000, 0.5f0, 1f-4)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]
chain = Chains(ch&#39;)</code></pre><hr/><pre><code class="language-julia hljs">yhats = naive_prediction(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><hr/><pre><code class="language-julia hljs">posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre><h3 id="Variation-Inference"><a class="docs-heading-anchor" href="#Variation-Inference">Variation Inference</a><a id="Variation-Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Variation-Inference" title="Permalink"></a></h3><p>In some cases MCMC method either do not work well or even the methods above take too long. For these cases BFlux currently implements Bayes-By-Backprop (see <a href="../../inference/vi/#BFlux.bbb"><code>bbb</code></a>); One shortcoming of the current implementation is that the variational family is constrained to a diagonal multivariate gaussian and thus any correlations between network parameters are set to zero. This can cause problems in some situations and plans exist to allow for more felxible covariance specifications.</p><pre><code class="language-julia hljs">q, params, losses = bbb(bnn, 10, 2_000; mc_samples = 1, opt = Flux.ADAM(), n_samples_convergence = 10)
ch = rand(q, 20_000)
posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« BFlux.jl a Bayesian extension to Flux.jl</a><a class="docs-footer-nextpage" href="../feedforward/">Example: Feedforward NN Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 7 December 2022 14:25">Wednesday 7 December 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

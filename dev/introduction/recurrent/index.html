<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example: Recurrent Neural Networks · BayesFlux.jl Documentation</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BayesFlux.jl Documentation</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../">BayesFlux.jl a Bayesian extension to Flux.jl</a></li><li><a class="tocitem" href="../linear-regression/">Example: Linear Regression</a></li><li><a class="tocitem" href="../feedforward/">Example: Feedforward NN Regression</a></li><li class="is-active"><a class="tocitem" href>Example: Recurrent Neural Networks</a></li></ul></li><li><span class="tocitem">Bayesian Neural Networks</span><ul><li><a class="tocitem" href="../../model/bnn/">Model Basics</a></li><li><a class="tocitem" href="../../model/sampling/">Prior and Posterior Predictive</a></li></ul></li><li><span class="tocitem">Likelihood Functions</span><ul><li><a class="tocitem" href="../../likelihoods/interface/">Interface</a></li><li><a class="tocitem" href="../../likelihoods/feedforward/">Feedforward Likelihoods</a></li><li><a class="tocitem" href="../../likelihoods/seqtoone/">Seq-to-One Likelihoods</a></li></ul></li><li><span class="tocitem">Network Priors</span><ul><li><a class="tocitem" href="../../priors/interface/">Interface</a></li><li><a class="tocitem" href="../../priors/gaussian/">Gaussian Prior</a></li><li><a class="tocitem" href="../../priors/mixturescale/">Mixture Scale Prior</a></li></ul></li><li><span class="tocitem">Inference</span><ul><li><a class="tocitem" href="../../inference/map/">MAP Estimation</a></li><li><a class="tocitem" href="../../inference/mcmc/">MCMC Estimation</a></li><li><a class="tocitem" href="../../inference/vi/">Variational Inference</a></li></ul></li><li><span class="tocitem">Initialisation</span><ul><li><a class="tocitem" href="../../initialise/init/">Initialisation</a></li></ul></li><li><span class="tocitem">Utils</span><ul><li><a class="tocitem" href="../../utils/recurrent/">Recurrent Architectures</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Introduction</a></li><li class="is-active"><a href>Example: Recurrent Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example: Recurrent Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/enweg/BayesFlux.jl/blob/main/docs/src/introduction/recurrent.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Example:-Recurrent-Neural-Networks"><a class="docs-heading-anchor" href="#Example:-Recurrent-Neural-Networks">Example: Recurrent Neural Networks</a><a id="Example:-Recurrent-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Recurrent-Neural-Networks" title="Permalink"></a></h1><p>Next to Dense layers, BayesFlux also implements RNN and LSTM layers. These two do require some additional care though, since the layout of the data must be adjusted. In general, the last dimension of <code>x</code> and <code>y</code> is always the dimension along which BayesFlux batches, which is also what Flux does. Thus, if we are in a seq-to-one setting then the sequences must be along the last dimension (here the third). To demonstrate this, let us simulate some AR1 data</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>BayesFlux currently only implements univariate regression problems (a single dependent variable) and for recurrent structures only seq-to-one type of settings. This can be extended by the user. For this see <a href="../../likelihoods/interface/#BayesFlux.BNNLikelihood"><code>BNNLikelihood</code></a></p></div></div><pre><code class="language-julia hljs">Random.seed!(6150533)
gamma = 0.8
N = 500
burnin = 1000
y = zeros(N + burnin + 1)
for t=2:(N+burnin+1)
    y[t] = gamma*y[t-1] + randn()
end
y = Float32.(y[end-N+1:end])</code></pre><p>Just like in the FNN case, we need a network structure and its constructor, a prior on the network parameters, a likelihood with a prior on the additional parameters introduced by the likelihood, and an initialiser. Note how most things are the same as for the FNN case, with the differences being the actual network defined and the likelihood.</p><pre><code class="language-julia hljs">net = Chain(RNN(1, 1), Dense(1, 1))  # last layer is linear output layer
nc = destruct(net)
like = SeqToOneNormal(nc, Gamma(2.0, 0.5))
prior = GaussianPrior(nc, 0.5f0)
init = InitialiseAllSame(Normal(0.0f0, 0.5f0), like, prior)</code></pre><p>We are given a single sequence (time series). To exploit batching and to not always have to feed through the whole sequence, we will split the single sequence into overlapping subsequences of length 5 and store these in a tensor. Note that we add 1 to the subsequence length, because the last observation of each subsequence will be our training observation to predict using the fist five items in the subsequence.</p><pre><code class="language-julia hljs">x = make_rnn_tensor(reshape(y, :, 1), 5 + 1)
y = vec(x[end, :, :])
x = x[1:end-1, :, :]</code></pre><p>We are now ready to create the BNN and find the MAP estimate. The MAP will be used to check whether the overall network structure makes sense (does provide at least good point estimates).</p><pre><code class="language-julia hljs">bnn = BNN(x, y, like, prior, init)
opt = FluxModeFinder(bnn, Flux.RMSProp())
θmap = find_mode(bnn, 10, 1000, opt)</code></pre><p>When checking the performance we need to make sure to feed the sequences through the network observation by observation:</p><pre><code class="language-julia hljs">nethat = nc(θmap)
yhat = vec([nethat(xx) for xx in eachslice(x; dims =1 )][end])
sqrt(mean(abs2, y .- yhat))</code></pre><p>The rest works just like before with some minor adjustments to the helper functions.</p><pre><code class="language-julia hljs">sampler = SGNHTS(1f-2, 1f0; xi = 1f0^2, μ = 10f0)
ch = mcmc(bnn, 10, 50_000, sampler)
ch = ch[:, end-20_000+1:end]
chain = Chains(ch&#39;)

function naive_prediction_recurrent(bnn, draws::Array{T, 2}; x = bnn.x, y = bnn.y) where {T}
    yhats = Array{T, 2}(undef, length(y), size(draws, 2))
    Threads.@threads for i=1:size(draws, 2)
        net = bnn.like.nc(draws[:, i])
        yh = vec([net(xx) for xx in eachslice(x; dims = 1)][end])
        yhats[:,i] = yh
    end
    return yhats
end</code></pre><hr/><pre><code class="language-julia hljs">yhats = naive_prediction_recurrent(bnn, ch)
chain_yhat = Chains(yhats&#39;)
maximum(summarystats(chain_yhat)[:, :rhat])</code></pre><hr/><pre><code class="language-julia hljs">posterior_yhat = sample_posterior_predict(bnn, ch)
t_q = 0.05:0.05:0.95
o_q = get_observed_quantiles(y, posterior_yhat, t_q)
plot(t_q, o_q, label = &quot;Posterior Predictive&quot;, legend=:topleft,
    xlab = &quot;Target Quantile&quot;, ylab = &quot;Observed Quantile&quot;)
plot!(x-&gt;x, t_q, label = &quot;Target&quot;)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../feedforward/">« Example: Feedforward NN Regression</a><a class="docs-footer-nextpage" href="../../model/bnn/">Model Basics »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Saturday 9 December 2023 11:12">Saturday 9 December 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
